# -*- coding: utf-8 -*-
"""copy_of_theme_generation_text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11U5miTmyKvw2C1xAIM9u3BwYA_C841u1
"""

import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.probability import FreqDist
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import os

# Function to preprocess the text
def preprocess_text(text):
    words = word_tokenize(text)
    stop_words = set(stopwords.words("english"))
    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]
    return " ".join(filtered_words)

# Read sentences from CSV or plain text file
def read_sentences1(file_path):
    if file_path.endswith(".xlsx"):
        df = pd.read_excel(file_path)# Replace 'your_column_name' with the actual column name in your CSV file.
        sentences = df[df.columns.values[0]].tolist()  # Replace 'your_column_name'
    elif file_path.endswith(".csv"):
        df = pd.read_csv(file_path)# Replace 'your_column_name' with the actual column name in your CSV file.
        sentences = df[df.columns.values[0]].tolist()  # Replace 'your_column_name'

    else:
        with open(file_path, "r") as file:
            sentences = file.readlines()
    return sentences

read_sentences1('challenges.xlsx')

def read_sentences():
  sentences = input().split(',')
  return sentences

# Topic modeling using LDA
def topic_modeling(sentences, num_topics):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(sentences)
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(X)
    return lda, vectorizer

# Assign sentences to themes based on dominant topics
def assign_sentences_to_themes(lda, vectorizer, sentences):
    theme_sentences = {i: [] for i in range(lda.n_components)}
    for sent in sentences:
        X_sent = vectorizer.transform([sent])
        topic_idx = lda.transform(X_sent).argmax()
        theme_sentences[topic_idx].append(sent)
    return theme_sentences

# Visualize themes using a bar chart and pie chart
def visualize_themes(theme_freq):
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.bar(theme_freq.keys(), theme_freq.values())
    plt.xlabel("Themes")
    plt.ylabel("Frequency")
    plt.title("Themes Frequency (Bar Chart)")

    plt.subplot(1, 2, 2)
    plt.pie(theme_freq.values(), labels=theme_freq.keys(), autopct="%1.1f%%")
    plt.title("Themes Distribution (Pie Chart)")

    plt.tight_layout()
    plt.savefig('plot.png')
    plt.plot()

# Extract the most relevant words for each topic
def extract_topic_keywords(lda, vectorizer, num_words=5):
    feature_names = vectorizer.get_feature_names_out()
    topic_keywords = []
    for topic_idx, topic in enumerate(lda.components_):
        top_words_idx = topic.argsort()[-num_words:][::-1]
        top_words = [feature_names[i] for i in top_words_idx]
        topic_keywords.append(top_words)
    return topic_keywords

# Main function
def main():
    nltk.download("punkt")
    nltk.download("stopwords")

    file = os.listdir('/content')
    f_file = [i for i in file if i.endswith('.xlsx' or '.csv')]
    file_path = f_file[0]

    num_topics = 5  # Number of themes to generate (you can adjust this)

    sentences = read_sentences()
    preprocessed_sentences = [preprocess_text(sent) for sent in sentences]

    lda, vectorizer = topic_modeling(preprocessed_sentences, num_topics)
    theme_sentences = assign_sentences_to_themes(lda, vectorizer, sentences)
    global theme_freq
    theme_freq = {f"Theme {i + 1}": len(theme_sentences[i]) for i in range(num_topics)}

    # Extract most relevant words for each topic
    topic_keywords = extract_topic_keywords(lda, vectorizer)

    # Print themes and their most relevant words
    global list_key
    list_key = []
    for theme, freq, keywords in zip(theme_freq.keys(), theme_freq.values(), topic_keywords):
        print(f"{theme}: {freq} sentences")
        print("Keywords:", ", ".join(keywords))
        list_key.append(keywords)
        print()

    #visualize_themes(theme_freq)
    return list_key

if __name__ == "__main__":
    main()

list_key

from nltk.corpus import wordnet
nltk.download('wordnet')

def find_theme_of_word(word):
    # Ensure NLTK resources are downloaded


    # Get the synsets (word meanings) of the given word
    word_synsets = wordnet.synsets(word.lower())

    # If no synsets found for the word, return None
    if not word_synsets:
        return None

    # Find the most common theme (hypernym) of the word's synsets
    themes = {}
    for synset in word_synsets:
        hypernyms = synset.hypernyms()
        for hypernym in hypernyms:
            themes[hypernym] = themes.get(hypernym, 0) + 1

    # Sort the themes by their frequency in descending order
    sorted_themes = sorted(themes.items(), key=lambda x: x[1], reverse=True)

    # Return the theme of the word (most frequent hypernym)
    theme_word = sorted_themes[0][0].lemmas()[0].name()
    return theme_word

l_theme = []
for i in list_key:
  print(i)
  common_theme = find_theme_of_word(i[3])
  print("Common Theme:", common_theme)
  l_theme.append(common_theme)

l_theme

theme_freq

for i in theme_freq:
  print(i)

for i in range(5):
  theme_freq[l_theme[i]] = theme_freq.pop(list(theme_freq)[0])

theme_freq

visualize_themes(theme_freq)

!pip install docx

!pip install python-docx

!pip install aspose-words

from docx import Document
from docx.shared import Inches
from docx.shared import Pt
import aspose.words as aw



document = Document()
document.add_heading("--Results--")

l = []
for i in range(5):
  txt = str(l_theme[i])+':'+str(list_key[i])
  l.append(txt)
l

for i in range(5):
  p = document.add_paragraph(str(l[i]))
p.style = document.styles['Normal']
r1 = p.add_run()
r1.add_picture('plot.png', width=Inches(6))
document.save("result.docx")
